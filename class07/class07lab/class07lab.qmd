---
title: "CLASS07LAB"
author: "Kenny Dang (PID:A18544481)"
format: pdf
toc: true
---

## Background 

Today we will begin our exploration of some important machine learning methods namely **clustering** and **dimensionality reduction**.

Let's make up some input data for clustering where we know what the natural "clusters" are

The function `rnorm()` can be useful here


```{r}
hist(rnorm(5000, 10))
```
>Q. generate 30 random numbers centered at +3 

```{r}
rnorm(30, 3)
rnorm(30, -3)
```

>Q. generate 30 random numbers centered at -3

```{r}
rnorm(30, -3)
```

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
tmp

x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```
```{r}
rev(letters)
```


## K-means clustering

The main function in "base R" for K-means clustering is called `kmeans()`:

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
tmp

x <- cbind(x=tmp, y=rev(tmp))

km <- kmeans(x, 2)
km
```




> Q. What component of the results object details the cluster sizes? 

```{r}
km$size
```
> Q. What components of the results object details the cluster centers?

```{r}
km$centers
```

> Q. What components of the results object details the cluster membership vector (i.e. our main result of which points lie in which cluster)?

```{r}
km$cluster
```


> Q. Plot our clustering results with points colored by cluster and also add the cluster centers as new points colored blue?


```{r}
plot(x, col=km$cluster)
points(km$centers, col ="blue", pch=15)
```


> Q. Run `kmeans()` again and this time produce 4 clusters (and call your result object `k4`) and make a results figure like above? 

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
tmp

x <- cbind(x=tmp, y=rev(tmp))

k4 <- kmeans(x,4)
plot(x, col=k4$cluster)
points(k4$centers, col ="blue", pch=15)



```

The metric
```{r}
km$tot.withinss
k4$tot.withinss
```


> Q. Let's try different number of K(centers) from 1 to 30 and see what the best result is? 

```{r}
i <- 1
ans <- NULL
for (i in 1:30) {
ans <- c(ans, kmeans(x, centers = i)$tot.withinss)
}

ans
```


```{r}
plot(ans, typ="o")
```

**Key-point:** K-means will impose a clustering structure on your data even if it is not there - it will always give you the answer you asked for even if that answer is silly!

## Hierarchical Clustering

The main function for Hierarchical Clustering is called `hclust()`. Unlike `kmeans()` (which does all the work for you) you can't just pass `hclust()` our raw input data. It needs a "distance matrix" like the one returned from the `dist()` function. 

```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```
To extract our cluster membership vector from a `hclust()`result object we have to "cut" our tree at a given hieght to yield separate "groups"/"branches".

```{r}
plot(hc)
abline(h=8, col="red", lty=2)
```

To do this we use the `cutree()` function on our `hclust()` object:

```{r}
grps <- cutree(hc, h=8)
grps
```
## should be two columns 
```{r}
tab <- table(grps, km$cluster)
tab[, c(2,1)]
```

## PCA of UK food data

Import  the dataset of food consumption in the UK:

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```


> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

There are 17 rows and 5 columns. I used the dim() function to find the answer for this. 

```{r}
dim(x)
```

One solution to set the row names is to do it by hand...

```{r}
rownames(x) <- x[, 1]
rownames(x)
```
To remove the first column I can use the minus index trick 

```{r}
x <- x[, -1]
x
```

A better way to do this is to set the row names to the first column with `read.csv()`

```{r}
x <- read.csv(url, row.names = 1)
x
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I prefer the second method more which has to do with the "read.csv()" function because that gives you a consistent answer. The first method we used does also give you the right answer but it can give you an inaccurate answer if you run it multiple times. The second method being "x <- x[,-1]".

## Spotting major differences and trends
Is difficult even in this wee 17D dataset...




```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))


```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))

```

> Q3: Changing what optional argument in the above barplot() function results in the following plot?

## setting the "beside" equal to FALSE



### Pairs plots and heatmaps

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)

```

> Q5. We can use the pairs() function to generate all pairwise plots for our countries. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The x axis and y axis represents the countries. Each small panel represents two countries simultaneously. Each dot represents a data point in regards to food consumption. Most of the panels appear to have a positive correlation since there seems to be a trend in which they are increasing together. In regards to the first row, the y-axis represents the England population and the x-axis represents the country that is right below it. So for example, the first small panel to the right of England is comparing data of Wales versus England. The rest of the panels on this visual follows a similar design. The diagonal panels are the country plotted against itself and so that is why those boxes show the country name instead of the data as the data would just be a straight line. If a point lies on the diagonal for a given plot, this means that the two countries being compared have the same value for that observation. But if the data point is above the diagonal, this means that the country on the y-axis has a larger value. And if the data point is below the diagonal, this means that the country on the x-axis has a larger value. 


```{r}
library(pheatmap)

pheatmap( as.matrix(x) )
```

> Q6. Based on the pairs and heatmap figures, which countries cluster together and what does this suggest about their food consumption patterns? Can you easily tell what the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The countries that cluster together are England, Wales, and Scotland. This means that they have very similar food consumption patterns across the categories in the dataset. The main differences between Norther Ireland and the other countries of the UK are that Northern Ireland has a more distinct cluster in comparison to the other three which means that its overall consumption differs systematically rather than by random variation. Northern Ireland has a more consistent food consumption pattern in comparison to the other countries and so this is why they have a more distinct cluster. 

## PCA to the rescue

The main PCA function in "base R" is called `prcomp()`. This function wants the transpose of our food data as input (i.e. the foods as columns and the countries as rows).

```{r}
pca <- prcomp(t(x))
pca
```

```{r}
summary(pca)
```
```{r}
attributes(pca)
```
To make one of main PCA result figures we turn to `pca$x` the scores along our new PCs. This is called "PC plot" or "score plot" or "Ordination plot" ...

```{r}
pca$x
```

```{r}
my_cols <- c("orange", "red", "blue", "darkgreen")
my_cols
```

> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
library (ggplot2)

# Create a data frame for plotting
df <- as.data.frame(pca$x)
df$Country <- rownames(df)

# Plot PC1 vs PC2 with ggplot
ggplot(pca$x)+
  aes(x = PC1, y = PC2, label = rownames(pca$x)) +
  geom_point(size = 3) +
  geom_text(vjust = -0.5) +
  xlim(-270, 500) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw()
```

> Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at the start of this document. 

```{r}
# Create a data frame for plotting
df <- as.data.frame(pca$x)
df$Country <- rownames(df)

# Plot PC1 vs PC2 with ggplot
ggplot(pca$x) +
  aes(x = PC1, y = PC2, label = rownames(pca$x)) +
  geom_point(size = 3, col = my_cols) +
  geom_text(vjust = -0.5) +
  xlim(-270, 500) +
  xlab("PC1") +
  ylab("PC2") +
  theme_bw()

```


```{r}
library (ggplot2)

ggplot (pca$x) +
  aes(PC1, PC2) +
  geom_point(col=my_cols)
```


The second major result figure is called a "loadings plot" of "variable contributions plot" or "weight plot"

```{r}
ggplot(pca$rotation) +
  aes(PC1, rownames(pca$rotation)) +
        geom_col()
```

> Q9. Generate a similar `loadings plot` for PC2. What two food groups feature predominately and what does PC2 mainly tell us about?

The two food groups that feature predominately are soft drinks (strong positive loading) and fresh potatoes (negative loading). PC2 tells us that diets with more soft drinks have high PC2 values while diets with more fresh potatoes have low PC2 values. It is essentially comparing processed/sugary beverages vs. more traditional staple foods. 
```{r}
ggplot(pca$rotation) +
  aes(PC2, rownames(pca$rotation)) +
        geom_col()
```



