---
title: "CLASS08 BREAST CANCER MINI PROJECT"
author: "Kenny Dang" 
format: pdf
toc: true
---

## Background

In today’s class we will be employing all the R techniques for data analysis that we have learned thus far – including the machine learning methods of clustering and PCA – to analyze real breast cancer biopsy data. 


```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
wisc.df
```


wee peak at the data 
```{r}
head(wisc.df,3)
```




> Q1. How many observations are in this dataset?

There are 700 observations in this dataset. 
```{r}
dim(wisc.df)
```





> Q2. How many of the observations have a malignant diagnosis?

212 of the observations have a malignant diagnosis. 
```{r}
sum(wisc.df$diagnosis == "M")

```

```{r}
table(wisc.df$diagnosis)
```



> Q3. How many variables/features in the data are suffixed with _mean?

10 of the variables/features in the data are suffixed with _mean.

```{r}
length (grep ("_mean", colnames(wisc.df), value = T))
```



We need to remopve the `diagnosis` column before we do any further analysis of this dataset - we don't want to pass this to PCA etc. We will save it as a separate wee vector that we can use later to compare our findings to those of experts. 

```{r}
wisc.data <- wisc.df[,-1]
wisc.data
diagnosis <- wisc.df$diagnosis
diagnosis
```



## Principal Component Analysis (PCA)

The main function in base R is called `prcomp()` we will use the optional argument 'scale=TRUE` here as the data columns/features/dimensions are on very different scales in the original data set. 

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
wisc.pr
```


```{r}
summary(wisc.pr)
```



```{r}
attributes(wisc.pr)
```

```{r}
library(ggplot2)

ggplot(wisc.pr$x) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)

About 44% of the original variance was captured by the first principal component. 
```{r}
wisc.pr$sdev[1]^2 / sum(wisc.pr$sdev^2)
```

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the original variance in the data.
```{r}
pve <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2)
which(cumsum(pve) >= 0.70)[1]

```



> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance in the data. 
```{r}
pve <- wisc.pr$sdev^2 / sum(wisc.pr$sdev^2)
which(cumsum(pve) >= 0.90)[1]
```
> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The plot is hard to read because it is very crowded. There are a lot of arrows and text and everything seems to mixed together instead of being separated out nicely. The individual features such as radius and perimeter are difficult to distinguish. Most of the arrows are pointing in the same direction which indicates that many of the variables are correlated and contribute to the PCs. Overall, the plot is hard to read due to the amount of arrows and text overlaps. 

```{r}
biplot(wisc.pr)
```




> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

In the plot for principal components 1 and 3, the data points are more overlapping than the plot for principal components 1 and 2. PC1 separates the diagnoses in both plots as the benign samples cluster to the right (representing positive PC1) and the malignant samples cluster to the left (representing negative PC1). This separation is seen in both plots. In the PC2 vs PC1 graph, there is some vertical spread and structure but there is not a lot of separation. In the PC3 vs PC1 graph, PC3 contributes even less visible separation between the benign  samples and the malignant samples. PC2 and PC3 capture within-group variation, not diagnostic differences. Overall, these plots are easier to interpret the loading biplot graphs since the ggplot graphs have clear axes and easy to see color coding and there is no overwhelming text on the graph itself.

## repeat for components 1 and 3
```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col= diagnosis) +
  geom_point()
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells you how much tihs original feature contributes to the first PC. Are there any features with larger contributions than this one? 

No, there appears to not be any features with a larger contribution than "concave.points_mean" which has a value of -0.26085376. This feature has a strong influence on PC1 as it is the largest loading in absolute value among all the other features. This feature in particular is the main driver of variation captured by PC1.

```{r}
wisc.pr$rotation[,1]
```



## 4. Hierarchical clustering

The goal of this section is to do hierarchical clustering of the original data to see if there is any obvious grouping into malignant and benign clusters. 

In short, these results are not good!

First we will scale our `wisc.data` then calculate a distance.matrix, then pass to `hclust()`:


```{r}
wisc.dist <- dist (scale(wisc.data))
wisc.dist
wisc_hclust <- hclust(wisc.dist)
wisc_hclust
plot(wisc_hclust)

```
> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

The height is 15 which will provide a clustering model with 4 clusters. 

```{r}
plot(wisc_hclust)
abline(h=15, col = "red", lty = 2)
```



```{r}
wisc.hclust.clusters <- cutree(wisc_hclust, k=2)
plot(wisc.hclust.clusters)
table (wisc.hclust.clusters)
```
> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning?

The method that provides my favorite results for the same data.dist dataset is "ward.D2" because it minimizes within-cluster variance, the dendrogram shows clear splits, and it is easy to interpret.

#Combining methods

The idea here is that I can take my new variables (i.e. the scores on the PCs `wisc.pr$x`) that are better descriptors of the data-set than the original features (i.e. the 30 columns in `wisc.data`) and use these as a basis for clustering.

```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pc.dist , method="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table (grps)
```
> Q13. How well does the newly created hclust model with two clusters separate out the two "M" and "B" diagnoses?

The two-cluster hierachical model separates malignant and benign diagnoses pretty well. One cluster is made up of malignant samples while the other is made up of benign samples with a little overlap. this shows that the clustering captures the main diagnostic structure in the data even though the structure is not perfect. 

```{r}
wis <- cutree(wisc.pr.hclust, k=2)
plot(wis)
```


I can now run `table()` with both my clustering `grps` and the expert `diagnosis`

```{r}
table(grps, diagnosis)
```

Our cluster "1" has a 179 "M" diagnosis
Our cluster "2" has 333 "B" diagnosis

179 TP
24 FP
333 TN
33 FN



```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
wisc.pr.hclust.clusters
```

> Q14. How well do the hierarchical clustering models you created in the previous sections (i.e. without first doing PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.hclust.clusters) and wisc.pr.hclust.clusters) with the vector containing the actual diagnoses.

The values between the two models are pretty different from each other. Clustering without PCA will result in heavily mixed clusters. Clustering with PCA will result in improved separation of benign and malignant samples. Overall, PCA helps reduce noise and redundancy which makes the diagnostic structure easier for hierarchical clustering to detect. 

```{r}
table(wisc.hclust.clusters, diagnosis)
```
```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```


Sensitivity: TP/(TP+FN)

```{r}
179/(179+33)
```

SpecificityL TN/(TN+FP)

```{r}
333/(333+24)
```

## Prediction

We can use our PCA model for prediction 

> Q16. Which of these new patients should we prioritize for follow up based on your results?

The patient that should be prioritized for follow up is patient 2 because their PC scores place them among samples that are mostly malignant. Meanwhile, patient 1 clusters with benign cases and seems to be a lower risk in comparison. The plot shows that patient 2 sits on the left which is where the malignant cases are clustered while patient 1 sits on the right which is where the benign cases are clustered. This means that patient 2 should be prioritized. 

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc

```


```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

